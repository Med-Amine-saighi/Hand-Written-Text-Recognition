{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-05-11T23:32:58.665765Z","iopub.status.busy":"2022-05-11T23:32:58.665431Z","iopub.status.idle":"2022-05-11T23:32:59.927683Z","shell.execute_reply":"2022-05-11T23:32:59.926829Z","shell.execute_reply.started":"2022-05-11T23:32:58.665665Z"},"trusted":true},"outputs":[],"source":["#!git clone https://github.com/githubharald/CTCDecoder.git"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:12.051157Z","iopub.status.busy":"2022-05-11T23:33:12.050904Z","iopub.status.idle":"2022-05-11T23:33:16.094487Z","shell.execute_reply":"2022-05-11T23:33:16.093727Z","shell.execute_reply.started":"2022-05-11T23:33:12.051121Z"},"trusted":true},"outputs":[],"source":["import albumentations\n","import torch\n","import numpy as np\n","from torchvision.utils import make_grid\n","from PIL import Image\n","from PIL import ImageFile\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import string\n","from skimage import io\n","import os\n","import glob\n","\n","from ctc_decoder import best_path, beam_search\n","from torchvision import transforms\n","import torch\n","import numpy as np\n","import pandas as pd\n","from torch.nn.utils.rnn import pack_sequence\n","from torch.nn.utils.rnn import pad_sequence\n","import torchvision.models as models\n","from torch.utils.model_zoo import load_url\n","from sklearn import preprocessing\n","from sklearn import model_selection\n","from sklearn import metrics\n","import cv2\n","import torch\n","from  torch import nn\n","from torch.nn import functional as F\n","from pprint import  pprint"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:16.096896Z","iopub.status.busy":"2022-05-11T23:33:16.096643Z","iopub.status.idle":"2022-05-11T23:33:16.20752Z","shell.execute_reply":"2022-05-11T23:33:16.206828Z","shell.execute_reply.started":"2022-05-11T23:33:16.096852Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\khamm\\AppData\\Local\\Temp\\ipykernel_17700\\1842511682.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  targets_enc  = np.array(targets_enc )\n"]}],"source":["aymen_csv = pd.read_csv('resWords.csv')\n","aymen_csv[\"image_path\"] = aymen_csv[\"IMAGE_ID\"].apply(lambda x: f\"Words/\"+x)\n","upper = list(string.ascii_uppercase)\n","numbers = [i for i in range(10)]\n","aymen_csv[\"LABEL TYPE\"] = aymen_csv[\"LABEL\"].apply(lambda x: \"phrase\" if x[0] in upper else \"numbers\")\n","train_csv = aymen_csv\n","train_csv = train_csv[['IMAGE_ID', 'LABEL','image_path']]\n","train_csv = train_csv.reset_index(drop=True)\n","train_csv = train_csv[~train_csv['IMAGE_ID'].isin(['Amount in numbers_000007.jpg',\n","                              'Amount in numbers_000017.jpg', \n","                              'Amount in numbers_000038.jpg',\n","                              'Amount in numbers_000043.jpg',\n","                              'Amount in numbers_000057.jpg',\n","                              'Amount in numbers_000330.jpg',\n","                              'Amount in numbers_000477.jpg',\n","                              'Amount in numbers_000590.jpg', \n","                              'Amount in numbers_000654.jpg', \n","                              'Amount in numbers_000738.jpg',\n","                              'Amount in numbers_000917.jpg',\n","                              'Amount in numbers_001238.jpg', \n","                              'Amount in numbers_001274.jpg', \n","                              'Amount in numbers_001348.jpg', 'Amount in numbers_001377.jpg', 'Amount in numbers_001439.jpg', 'Amount in numbers_001442.jpg', 'Amount in numbers_001616.jpg', 'Amount in numbers_001753.jpg', 'Amount in numbers_001804.jpg', 'Amount in numbers_001969.jpg', 'Amount in numbers_002103.jpg', 'Amount in numbers_002248.jpg', 'Amount in numbers_002253.jpg', 'Amount in numbers_002502.jpg', 'Amount in numbers_002523.jpg', 'Amount in numbers_002581.jpg', 'Amount in numbers_002639.jpg', 'Amount in numbers_002915.jpg', 'Amount in numbers_002951.jpg', 'Amount in numbers_003026.jpg', 'Amount in numbers_003059.jpg', 'Amount in numbers_003172.jpg', 'Amount in numbers_003258.jpg', 'Amount in numbers_003527.jpg', 'Amount in numbers_003570.jpg', 'Amount in numbers_003638.jpg', 'Amount in numbers_003700.jpg', 'Amount in numbers_003743.jpg', 'Amount in numbers_003927.jpg', 'Amount in numbers_003949.jpg', 'Amount in numbers_003957.jpg', 'Amount in numbers_004016.jpg', 'Amount in numbers_004045.jpg', 'Amount in numbers_004125.jpg', 'Amount in numbers_004144.jpg', 'Amount in numbers_004184.jpg', 'Amount in numbers_004196.jpg', 'Amount in numbers_004261.jpg', 'Amount in numbers_004315.jpg', 'Amount in numbers_004328.jpg', 'Amount in numbers_004366.jpg', 'Amount in numbers_004417.jpg', 'Amount in numbers_004477.jpg', 'Amount in numbers_004502.jpg', 'Amount in numbers_004622.jpg', 'Amount in numbers_004645.jpg', 'Amount in numbers_004679.jpg', 'Amount in numbers_004906.jpg', 'Amount in numbers_004993.jpg', 'Amount in numbers_005037.jpg', 'Amount in numbers_005190.jpg', 'Amount in numbers_005203.jpg', 'Amount in numbers_005247.jpg', 'Amount in numbers_005528.jpg', 'Amount in numbers_005573.jpg', 'Amount in numbers_005645.jpg', 'Amount in numbers_005696.jpg', 'Amount in numbers_005776.jpg', 'Amount in numbers_005799.jpg', 'Amount in numbers_005946.jpg', 'Amount in numbers_006073.jpg', 'Amount in numbers_006123.jpg', 'Amount in numbers_006158.jpg', 'Amount in numbers_006361.jpg', 'Amount in numbers_006406.jpg', 'Amount in numbers_006700.jpg', 'Amount in numbers_006951.jpg', 'Amount in numbers_006977.jpg'])]\n","train_csv = train_csv.reset_index(drop=True)\n","image_paths = train_csv['image_path']\n","image_files = train_csv['LABEL']\n","\n","targets_orig = [x for x in image_files]\n","\n","targets = [[c for c in x] for x in targets_orig]\n","\n","targets_flat = [c for clist in targets for c in clist]\n","\n","lbl_encoder = preprocessing.LabelEncoder()\n","lbl_encoder.fit(targets_flat)\n","\n","targets_enc  = [lbl_encoder.transform(x) for x in targets]\n","targets_enc  = np.array(targets_enc ) \n","train_csv[\"LABEL ENCODED\"] = targets_enc"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:16.574447Z","iopub.status.busy":"2022-05-11T23:33:16.573916Z","iopub.status.idle":"2022-05-11T23:33:16.631146Z","shell.execute_reply":"2022-05-11T23:33:16.630431Z","shell.execute_reply.started":"2022-05-11T23:33:16.574408Z"},"trusted":true},"outputs":[],"source":["class config:\n","    BATCH_SIZE = 64\n","    IMAGE_WIDTH = 500\n","    IMAGE_HEIGHT = 100\n","    NUM_WORKERS = 2\n","    EPOCHS = 130\n","    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu' )"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.549286Z","iopub.status.busy":"2022-05-11T23:33:21.548905Z","iopub.status.idle":"2022-05-11T23:33:21.557198Z","shell.execute_reply":"2022-05-11T23:33:21.556344Z","shell.execute_reply.started":"2022-05-11T23:33:21.549249Z"},"trusted":true},"outputs":[],"source":["class AddGaussianNoise(object):\n","    def __init__(self, mean=0., std=1.):\n","        self.std = std\n","        self.mean = mean\n","        \n","    def __call__(self, tensor):\n","        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n","    \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.558856Z","iopub.status.busy":"2022-05-11T23:33:21.558593Z","iopub.status.idle":"2022-05-11T23:33:21.568556Z","shell.execute_reply":"2022-05-11T23:33:21.567843Z","shell.execute_reply.started":"2022-05-11T23:33:21.558823Z"},"trusted":true},"outputs":[],"source":["train_transform = transforms.Compose([\n","    \n","    transforms.ToTensor(),\n","    #transforms.RandomApply([AddGaussianNoise()], p=0.15),\n","    transforms.RandomPerspective(distortion_scale=0.2, p=0.15),\n","    transforms.Resize(size=(config.IMAGE_HEIGHT,config.IMAGE_WIDTH)),\n","    \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n","    \n","])\n","\n","test_transform = transforms.Compose([\n","    \n","    transforms.ToTensor(),\n","    transforms.Resize(size=(config.IMAGE_HEIGHT,config.IMAGE_WIDTH)),\n","    \n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n","    \n","])"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.570824Z","iopub.status.busy":"2022-05-11T23:33:21.570213Z","iopub.status.idle":"2022-05-11T23:33:21.579429Z","shell.execute_reply":"2022-05-11T23:33:21.578574Z","shell.execute_reply.started":"2022-05-11T23:33:21.570787Z"},"trusted":true},"outputs":[],"source":["class dataset:\n","    ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","    class ClassificationDataset:\n","        def __init__(self, dataframe, transform=None):\n","            self.dataframe = dataframe\n","            #self.aug = albumentations.Compose([albumentations.Normalize(always_apply=True)])\n","            self.transform = transform\n","\n","        def __len__(self):\n","            return len(self.dataframe)\n","\n","        def __getitem__(self, item):\n","            image = Image.open(self.dataframe['image_path'][item]).convert(\"RGB\")\n","            targets = self.dataframe['LABEL ENCODED'][item]\n","            \n","            \n","            \n","            if self.transform:\n","                image = self.transform(image)\n","            \n","            image = np.array(image)\n","            #augmented = self.aug(image=image)\n","            #image = augmented[\"image\"]\n","\n","            return {\n","                \"images\": torch.tensor(image, dtype=torch.float),\n","                \"targets\": torch.tensor(targets, dtype=torch.long)\n","            }"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.581975Z","iopub.status.busy":"2022-05-11T23:33:21.581651Z","iopub.status.idle":"2022-05-11T23:33:21.725455Z","shell.execute_reply":"2022-05-11T23:33:21.724741Z","shell.execute_reply.started":"2022-05-11T23:33:21.58194Z"},"trusted":true},"outputs":[],"source":["class CaptchaModel(nn.Module):\n","    def __init__(self, num_chars):\n","        super(CaptchaModel, self).__init__()\n","        self.conv_1 = nn.Conv2d(3, 128, kernel_size=(3, 3), padding=(1, 1))\n","        self.pool_1 = nn.MaxPool2d(kernel_size=(2, 2))\n","        self.conv_2 = nn.Conv2d(128, 64, kernel_size=(3, 3), padding=(1, 1))\n","        self.pool_2 = nn.MaxPool2d(kernel_size=(2, 2))\n","\n","        self.linear1 = nn.Linear(1600, 64)\n","        self.drop_1 = nn.Dropout(0.2)\n","\n","        self.gru = nn.GRU(64, 32, bidirectional=True, num_layers=2, dropout=0.25, batch_first=True)\n","        self.output = nn.Linear(64, num_chars + 1)\n","\n","    def forward(self, images,lens, targets=None):\n","        bs, c, h, w = images.size()\n","        #print(bs, c, h, w)\n","        x = F.relu(self.conv_1(images))\n","        x = self.pool_1(x)\n","        x = F.relu(self.conv_2(x))\n","        x = self.pool_2(x) # 1, 64, 18, 75\n","        x = x.permute(0, 3, 1, 2) # 1, 75 , 64, 18\n","        x = x.view(bs, x.size(1), -1)\n","        x = self.linear1(x)\n","        x = self.drop_1(x)\n","        #print(x.size()) # torch.Size([1, 75, 64]) -> we have 75 time steps and for each time step we have 64 values\n","        x, _ = self.gru(x)\n","        x = self.output(x)\n","        x = x.permute(1, 0, 2) # bs, time steps, values -> CTC LOSS expects it to be\n","\n","        if targets is not None:\n","            log_probs = F.log_softmax(x, 2)\n","            input_lengths = torch.full(\n","                size=(bs,), fill_value=log_probs.size(0), dtype=torch.int32\n","            )\n","\n","            target_lengths = torch.tensor(\n","                lens, dtype=torch.int32\n","            )\n","            loss = nn.CTCLoss(blank=len(lbl_encoder.classes_))(\n","                log_probs, targets, input_lengths, lens\n","            )\n","            return x, loss\n","\n","        return x, None"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.727307Z","iopub.status.busy":"2022-05-11T23:33:21.727016Z","iopub.status.idle":"2022-05-11T23:33:21.737103Z","shell.execute_reply":"2022-05-11T23:33:21.736386Z","shell.execute_reply.started":"2022-05-11T23:33:21.727271Z"},"trusted":true},"outputs":[],"source":["def collate(batch):\n","    images, words = [b.get('images') for b in batch], [b.get('targets') for b in batch]\n","    images = torch.stack(images, 0)\n","\n","    lens = [len(item['targets']) for item in batch]\n","    \n","    list_of_chars_digitized = [word for word in words]\n","    lengths = len(list_of_chars_digitized)\n","\n","    lengths = torch.tensor(lengths, dtype=torch.long)\n","    lens = torch.tensor(lens, dtype=torch.long)\n","\n","    for i in range(lengths.item()):\n","        targets = torch.cat(words)\n","\n","    \n","    \n","    return  images,targets, lens"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.738962Z","iopub.status.busy":"2022-05-11T23:33:21.738301Z","iopub.status.idle":"2022-05-11T23:33:21.750321Z","shell.execute_reply":"2022-05-11T23:33:21.749454Z","shell.execute_reply.started":"2022-05-11T23:33:21.738927Z"},"trusted":true},"outputs":[],"source":["class engine:\n","    def train_fn(model, data_loader, optimizer):\n","        model.train()\n","        fin_loss = 0\n","        tk0 = tqdm(data_loader, total=len(data_loader))\n","        for xb, yb, lens in tk0:\n"," \n","            xb = xb.to(config.DEVICE)\n","            yb = yb.to(config.DEVICE)\n","            lens = lens.to(config.DEVICE)\n","            \n","            optimizer.zero_grad()\n","            _, loss = model(xb, lens, yb)\n","            loss.backward()\n","            optimizer.step()\n","            fin_loss += loss.item()\n","        return fin_loss / len(data_loader)\n","\n","\n","    def eval_fn(model, data_loader):\n","        model.eval()\n","        fin_loss = 0\n","        fin_preds = []\n","        with torch.no_grad():\n","            tk0 = tqdm(data_loader, total=len(data_loader))\n","            for xb, yb, lens in tk0:\n","\n","                \n","                xb = xb.to(config.DEVICE)\n","                yb = yb.to(config.DEVICE)\n","                lens = lens.to(config.DEVICE)\n","                \n","                batch_preds, loss = model(xb, lens, yb)\n","                fin_loss += loss.item()\n","                fin_preds.append(batch_preds.detach().cpu())\n","            return fin_preds, fin_loss / len(data_loader)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.752455Z","iopub.status.busy":"2022-05-11T23:33:21.751938Z","iopub.status.idle":"2022-05-11T23:33:21.760469Z","shell.execute_reply":"2022-05-11T23:33:21.759623Z","shell.execute_reply.started":"2022-05-11T23:33:21.752418Z"},"trusted":true},"outputs":[],"source":["def decode_predictions(preds, encoder):\n","\n","    preds = preds.permute(1, 0, 2) \n","    preds = torch.softmax(preds, 2)\n","\n","    preds = torch.argmax(preds, 2)\n","\n","    preds = preds.numpy()\n","\n","    cap_preds = []\n","    \n","    for j in range(preds.shape[0]):\n","        temp = []\n","        for k in preds[j, :]:\n","            if k == len(lbl_encoder.classes_):\n","                temp.append(\"Â°\")\n","            else:\n","                p = encoder.inverse_transform([k])[0]\n","                temp.append(p)\n","        tp = \"\".join(temp)\n","        cap_preds.append(tp)\n","    return cap_preds"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.762554Z","iopub.status.busy":"2022-05-11T23:33:21.762065Z","iopub.status.idle":"2022-05-11T23:33:21.771129Z","shell.execute_reply":"2022-05-11T23:33:21.770313Z","shell.execute_reply.started":"2022-05-11T23:33:21.762518Z"},"trusted":true},"outputs":[],"source":["def decode_predictions_2(preds, encoder):\n","    preds = torch.softmax(preds, 2)\n","    preds = preds.detach().cpu().numpy()\n","    for i in range(preds.shape[1]):\n","        aux =  preds[:,i,:] \n","        print(f'Beam search: \"{beam_search(aux,lbl_encoder.classes_)}\"')\n","        break\n","    \n","    return None"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.792023Z","iopub.status.busy":"2022-05-11T23:33:21.791483Z","iopub.status.idle":"2022-05-11T23:33:21.805242Z","shell.execute_reply":"2022-05-11T23:33:21.804589Z","shell.execute_reply.started":"2022-05-11T23:33:21.791987Z"},"trusted":true},"outputs":[],"source":["def run_training():\n","    (\n","        train,\n","        test,\n","        _,\n","        test_targets_orig,\n","    ) = model_selection.train_test_split(\n","        train_csv, targets_orig, test_size=0.1, random_state=42\n","    )\n","\n","    train = train.reset_index(drop=True)\n","    test = test.reset_index(drop=True)\n","    \n","    train_dataset = dataset.ClassificationDataset(\n","        dataframe=train,\n","        transform=train_transform\n","        )\n","    train_loader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=config.BATCH_SIZE,\n","            shuffle=True,\n","            collate_fn=collate\n","        )\n","    test_dataset = dataset.ClassificationDataset(\n","        dataframe=test,\n","        transform=test_transform\n","        \n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=config.BATCH_SIZE,\n","        shuffle=False,\n","        collate_fn=collate\n","    )\n","\n","    model = CaptchaModel(num_chars=len(lbl_encoder.classes_))\n","    model.to(config.DEVICE)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer, factor=0.8, patience=5, verbose=True\n","    )\n","    best_score = 0.0 \n","    best_loss = np.inf\n","\n","    for epoch in range(config.EPOCHS):\n","        train_loss = engine.train_fn(model, train_loader, optimizer)\n","        valid_preds, valid_loss = engine.eval_fn(model, test_loader)\n","        valid_cap_preds = []\n","        for vp in valid_preds:\n","            decode_predictions_2(vp ,lbl_encoder )\n","            break\n","            \n","        for vp in valid_preds:\n","            current_preds = decode_predictions(vp, lbl_encoder)\n","            valid_cap_preds.extend(current_preds)\n","\n","        pprint(list(zip(test_targets_orig, valid_cap_preds))[0:6])\n","        print(f\"EPOCH: {epoch}   ,    train_loss={train_loss},    valid_loss={valid_loss}\")\n","\n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            print(\"=> saving checkpoint\")\n","            torch.save(model.state_dict(),'Words_best_score.pt')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-05-11T23:33:21.807003Z","iopub.status.busy":"2022-05-11T23:33:21.806493Z"},"trusted":true},"outputs":[],"source":["run_training()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.12 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"vscode":{"interpreter":{"hash":"2f091e354277450b0b6b718b66946b2c39d7f0f8fea3c545363b3468ad8d96c1"}}},"nbformat":4,"nbformat_minor":4}
